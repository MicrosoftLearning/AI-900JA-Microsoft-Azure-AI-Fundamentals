{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 顔の検出と分析\r\n",
        "\r\n",
        "Computer Vision ソリューションで人間の顔を検出、分析、識別するためには、通常、人工知能 (AI) ソリューションが必要です。たとえば、小売店 Northwind Traders が「スマート ストア」を開設するとします。このスマート ストアでは、AI サービスが店舗を監視して、支援が必要な買い物客を特定し、店員に支援を指示します。これを実現する 1 つの方法は、顔の検出と分析を行うことです。画像に写っている顔があるかどうかを判断し、ある場合はその特徴を分析します。\r\n",
        "\r\n",
        "![顔を分析しているロボット](./images/face_analysis.jpg)\r\n",
        "\r\n",
        "## Face Cognitive Services を使用して顔を検出する\r\n",
        "\r\n",
        "Northwind Traders が構築するスマート ストア システムでは、買い物客を検出して顔の特徴を分析する機能が必要です。Microsoft Azure の Cognitive Services にある **Face** を使用すると、これを行うことができます。\r\n",
        "\r\n",
        "### Cognitive Services リソースを作成する\r\n",
        "\r\n",
        "Azure サブスクリプションに **Cognitive Services** のリソースを作成することから始めます。\r\n",
        "\r\n",
        "> **注**: Cognitive Services リソースが既にある場合は、Azure portal で**クイック スタート**ページを開き、キーとエンドポイントを以下のセルにコピーするだけで作成できます。それ以外の場合は、以下の手順に従って作成してください。\r\n",
        "\r\n",
        "1. ブラウザーの新しいタブで Azure portal (https://portal.azure.com) を開き、Microsoft アカウントでサインインします。\r\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Cognitive Services* を検索して、以下の設定で **Cognitive Services** リソースを作成します。\r\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\r\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\r\n",
        "    - **リージョン**: *利用可能な任意のリージョンを選択します*。\r\n",
        "    - **名前**: *一意の名前を入力します*。\r\n",
        "    - **価格レベル**: S0\r\n",
        "    - **注意事項を読み理解しました**: 選択されています。\r\n",
        "3. デプロイが完了するまで待ちます。そのあと Cognitive Services リソースに移動し、「**概要**」 ページでリンクをクリックしてサービスのキーを管理します。クライアント アプリケーションから Cognitive Services リソースに接続するには、エンドポイントとキーが必要です。\r\n",
        "\r\n",
        "### Cognitive Services リソースのキーとエンドポイントを取得する\r\n",
        "\r\n",
        "クライアント アプリケーションで Cognitive Services リソースを使用するには、そのエンドポイントと認証キーが必要です。\r\n",
        "\r\n",
        "1. Azure portalで、Cognitive Services リソースの 「**キーとエンドポイント**」 ページからリソースの 「**キー 1**」 の値をコピーし、以下のコードに貼り付けます (**YOUR_COG_KEY** と置き換える)。\r\n",
        "\r\n",
        "2. リソースの**エンドポイント**をコピーして以下のコードに貼り付けます (**YOUR_COG_ENDPOINT** と置き換える)。\r\n",
        "\r\n",
        "3. セルの左上にある 「セルの実行」 <span>&#9655;</span> ボタンをクリックして、以下のセルのコードを実行します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693964655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cognitive Services リソースを作成したので、Face サービスを使用して店舗の中にいる人の顔を検出できるようになりました。\r\n",
        "\r\n",
        "例を確認するには、以下のコード セルを実行してください。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# 顔検出クライアントを作成します\r\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# 画像を開きます\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 顔を検出します\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# 顔を表示します (python_code/faces.py のコード)\r\n",
        "faces.show_faces(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "検出した各顔には一意の ID が割り当てられるため、検出した個々の顔をアプリケーションで識別できます。\r\n",
        "\r\n",
        "以下のセルを実行して、さらに何人かの買い物客の顔 ID を確認します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像を開きます\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 顔を検出します\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# 顔を表示します (python_code/faces.py のコード)\r\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 顔の属性を分析する\r\n",
        "\r\n",
        "Face は、単に顔を検出するだけではありません。顔の特徴や表情を分析して、年齢や感情状態を示唆することもできます。たとえば、以下のコードを実行して、買い物客の顔の属性を分析します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像を開きます\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 顔と、顔の特定の属性を検出します\r\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# 顔と属性を表示します (python_code/faces.py のコード)\r\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693971321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "画像から検出した買い物客の感情スコアによると、この客は買い物にかなり満足しているようです。\r\n",
        "\r\n",
        "## 似た顔を見つける \r\n",
        "\r\n",
        "検出した各顔に付けた顔 ID を使用して、各顔の検出結果を識別します。この ID を使用すると、検出した顔と同じような特徴を持つ顔を、以前に検出した複数の顔の中から見つけることができます。\r\n",
        "\r\n",
        "たとえば、以下のセルを実行して、画像に写っている買い物客と、別の画像に写っている複数の買い物客を比較し、一致する顔を見つけます。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像 1 の最初の顔 ID を取得します\r\n",
        "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_1_stream = open(image_1_path, \"rb\")\n",
        "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
        "face_1 = image_1_faces[0]\n",
        "\n",
        "# 2 番目の画像で複数の顔 ID を取得します\r\n",
        "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_2_stream = open(image_2_path, \"rb\")\n",
        "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
        "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
        "\n",
        "# 画像 1 の中のある顔に類似している顔を、画像 2 の複数の顔から見つけます\r\n",
        "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
        "\n",
        "# 画像 1 の顔を表示し、画像 2 の中から類似している顔を表示します (python_code/face.py のコード)\r\n",
        "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693972555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 顔を認識する\r\n",
        "\r\n",
        "これまで、Face を使用すると、顔とその特徴を検出し、互いに類似している 2 つの顔を識別できることを確認しました。さらに一歩進めるために、Face をトレーニングして特定の人物の顔を認識できる*顔認識*ソリューションを実装します。ソーシャル メディア アプリケーションで友人の写真に自動的にタグを付けたり、生体認証システムの一部として顔認識を使用したりするなど、さまざまなシナリオで役立ちます。\r\n",
        "\r\n",
        "これがどのように機能するかを確認するために、Northwind Traders 社では IT 部門の許可された従業員のみがセキュリティで保護されたシステムにアクセスできるように、顔認識を使用しようとしていると仮定します。\r\n",
        "\r\n",
        "まず、許可された複数の従業員を表す*人物のグループ*を作成します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "group_id = 'employee_group_id'\n",
        "try:\n",
        "    # Delete group if it already exists\n",
        "    face_client.person_group.delete(group_id)\n",
        "except Exception as ex:\n",
        "    print(ex.message)\n",
        "finally:\n",
        "    face_client.person_group.create(group_id, 'employees')\n",
        "    print ('Group created!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693973492
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*人物のグループ*を作成したので、このグループに含める従業員ごとに*人物*を追加し、各人物の複数の写真を登録します。これによって、Face が各人物の顔の特徴を学習できるようにします。理想的には、それらの画像は同じ人物をさまざまなポーズや、さまざまな表情で表現している必要があります。\r\n",
        "\r\n",
        "Wendell という従業員を追加し、その従業員の写真を 3 枚登録します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# ある人 (Wendell) をグループに追加します\r\n",
        "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
        "\n",
        "# Wendell の写真を取得します\r\n",
        "folder = os.path.join('data', 'face', 'wendell')\n",
        "wendell_pics = os.listdir(folder)\n",
        "\n",
        "# 写真を登録します\r\n",
        "i = 0\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for pic in wendell_pics:\n",
        "    # Add each photo to person in person group\n",
        "    img_path = os.path.join(folder, pic)\n",
        "    img_stream = open(img_path, \"rb\")\n",
        "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
        "\n",
        "    # Display each image\n",
        "    img = Image.open(img_path)\n",
        "    i +=1\n",
        "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693976898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "人物を追加し、写真を登録すると、Face をトレーニングして各人物を認識できるようになります。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "face_client.person_group.train(group_id)\n",
        "print('Trained!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693977046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで、モデルをトレーニングできたので、モデルを使用して画像の中から認識した顔を識別できるようになります。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 番目の画像で複数の顔 ID を取得します\r\n",
        "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
        "\n",
        "# 認識した顔の名前を取得します\r\n",
        "face_names = {}\n",
        "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
        "for face in recognized_faces:\n",
        "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
        "    face_names[face.face_id] = person_name\n",
        "\n",
        "# 認識した顔を表示します\r\n",
        "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693994820
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 詳細情報\r\n",
        "\r\n",
        "Face Cognitive Services の詳細については、「[Face ドキュメント](https://docs.microsoft.com/azure/cognitive-services/face/)」を参照してください。\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}